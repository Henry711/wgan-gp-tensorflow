{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN_working_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHAtK8zL5xkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        " \n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-5\n",
        "ALPHA_WEIGHT = 200\n",
        "DISCRIMINATOR_FACTOR = 2\n",
        " \n",
        "daccuracies = []\n",
        "gaccuracies = []"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaTx9Avs5_5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "1bfb74d9-1111-49df-ac5d-8ae33e6fdc86"
      },
      "source": [
        "def make_generator_model():\n",
        "#input layer\n",
        "   model = tf.keras.Sequential()\n",
        "   model.add(Dense(4*4*256,use_bias = False, input_shape = (100,)))\n",
        "   model.add(layers.BatchNormalization())\n",
        "   model.add(layers.LeakyReLU(0.2))\n",
        "   model.add(layers.Reshape((4,4,256)))\n",
        "   assert model.output_shape == (None, 4, 4, 256)\n",
        " \n",
        "#layer 1\n",
        " \n",
        "   model.add(layers.UpSampling2D((2,2)))\n",
        "   model.add(layers.Conv2DTranspose(128,\n",
        "                                    (3, 3),\n",
        "                                    strides=(1, 1),\n",
        "                                    padding='same',\n",
        "                                    use_bias = False))\n",
        "   assert model.output_shape == (None, 8, 8, 128)#maybe wrong\n",
        "   model.add(layers.LeakyReLU(0.2))\n",
        " \n",
        "#layer 2\n",
        " \n",
        "   model.add(layers.UpSampling2D((2,2)))\n",
        "   model.add(layers.Conv2DTranspose(64,\n",
        "                                    (3,3),\n",
        "                                    strides=(1, 1),\n",
        "                                    padding='same',\n",
        "                                    use_bias = False))\n",
        "   assert model.output_shape == (None,16,16,64)\n",
        "   model.add(layers.LeakyReLU(0.2))\n",
        " \n",
        "#layer 3 / output layer\n",
        "   model.add(layers.UpSampling2D((2,2)))\n",
        "   model.add(layers.Conv2DTranspose(1,\n",
        "                                    (3,3),\n",
        "                                    strides=(1, 1),\n",
        "                                    padding='same',\n",
        "                                    use_bias = False,\n",
        "                                    activation='tanh'))   \n",
        "   assert model.output_shape == (None,32,32 ,1)\n",
        "   model.add(layers.Cropping2D((2,2)))\n",
        "   return model\n",
        " \n",
        "generator = make_generator_model()\n",
        "generator.summary()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.5, beta_2 = 0.9)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4096)              409600    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 8, 8, 128)         294912    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 64)        73728     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 1)         576       \n",
            "_________________________________________________________________\n",
            "cropping2d (Cropping2D)      (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 795,200\n",
            "Trainable params: 787,008\n",
            "Non-trainable params: 8,192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPR7b2PWmWNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "9055cd2c-3c6c-4a06-f9df-cceff593e891"
      },
      "source": [
        "def make_discriminator_model():\n",
        "# layer 1\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape = (28, 28, 1)))\n",
        "    model.add(layers.ZeroPadding2D((2,2)))\n",
        "    model.add(layers.Conv2D(64,\n",
        "                            (5,5),\n",
        "                            strides=(2,2),\n",
        "                            padding = 'same',\n",
        "                            use_bias = True))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        " \n",
        "#layer 2\n",
        "    model.add(layers.Conv2D(128,\n",
        "                            (5,5),\n",
        "                            strides=(2, 2),\n",
        "                            padding = 'same',\n",
        "                            use_bias = True))\n",
        "    model.add(layers.LeakyReLU(0.2))    \n",
        " \n",
        "#layer 3\n",
        " \n",
        "    model.add(layers.Conv2D(256,\n",
        "                            (5,5),\n",
        "                            strides=(2, 2),\n",
        "                            padding = 'same',\n",
        "                            use_bias = True))\n",
        "    model.add(layers.LeakyReLU(0.2))    \n",
        " \n",
        "#layer 4\n",
        " \n",
        "    model.add(layers.Conv2D(512,\n",
        "                            (5,5),\n",
        "                            strides=(2, 2),\n",
        "                            padding = 'same',\n",
        "                            use_bias = True,\n",
        "                            activation = 'linear'))\n",
        "    model.add(layers.LeakyReLU(0.2))   \n",
        "    model.add(layers.Flatten())     \n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        " \n",
        "discriminator = make_discriminator_model()\n",
        "discriminator.summary()\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = 3e-4, beta_1 = 0.5, beta_2 = 0.9)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d (ZeroPadding2 (None, 32, 32, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2049      \n",
            "=================================================================\n",
            "Total params: 4,305,409\n",
            "Trainable params: 4,305,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz2DI5XzrSso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.reduce_mean(real_output)\n",
        "    fake_loss = tf.reduce_mean(fake_output)\n",
        "    total_loss = fake_loss - real_loss\n",
        "    d = total_loss.numpy()\n",
        "    daccuracies.append(-d)\n",
        "    return total_loss"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgkNp4nrYP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    x = -tf.reduce_mean(fake_output)\n",
        "    t = x.numpy()\n",
        "    for i in range(DISCRIMINATOR_FACTOR):\n",
        "      gaccuracies.append(-t)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY0s-93XruXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9739ac69-386c-407b-db94-2b2b57c89055"
      },
      "source": [
        "(train_images,_), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images.astype(np.float32) - 127.5)/127.5\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9C1J2FBCjle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_penalty(real_images, fake_images, batch_size, alpha_weight):\n",
        "    # get the interpolated image\n",
        "    alpha = tf.random.uniform([batch_size,1,1,1],0.0,1.0)\n",
        "    diff = fake_images - real_images\n",
        "    interpolated = alpha*real_images + ((1-alpha)*fake_images)\n",
        "    # 2. Calculate the gradient\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(interpolated)\n",
        "      pred = discriminator(interpolated)\n",
        "    grads = tape.gradient(pred, interpolated)[0]\n",
        "    # 3. Calcuate the norm of the gradients\n",
        "    norm = ((tf.norm(grads)-0)**2)\n",
        "    gp1 = tf.reduce_mean(norm)\n",
        "    gp = alpha_weight*norm\n",
        "    return gp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Vf3zXHG_Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(image,batch_size, discriminator_factor,alpha_weight):\n",
        "    noise = tf.random.normal([batch_size,100])    \n",
        "    with tf.GradientTape() as gen_tape:\n",
        "       generated_images = generator(noise)\n",
        "       fake_output = discriminator(generated_images)\n",
        "       gen_loss = generator_loss(fake_output)  \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    for i in range(discriminator_factor):\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            generated_images = generator(noise)\n",
        "            real_output = discriminator(image)\n",
        "            fake_output = discriminator(generated_images)\n",
        "            gp = gradient_penalty(image, generated_images,batch_size, alpha_weight)\n",
        "            disc_loss = discriminator_loss(real_output, fake_output) \n",
        "            d_cost = disc_loss+gp\n",
        "        gradients_of_discriminator = disc_tape.gradient(d_cost, discriminator.trainable_variables)\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N8KAvFW2O_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_and_save_images(model, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "  predictions = tf.reshape(predictions, (9,28,28))\n",
        "  fig = plt.figure(figsize=(9,9))\n",
        "\n",
        "  for i in range((predictions.shape[0])):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(predictions[i,:,:] * 255)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxpSLiw1HOXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "seed = tf.random.normal([9, 100])\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch,BATCH_SIZE,DISCRIMINATOR_FACTOR, ALPHA_WEIGHT)\n",
        "      global i\n",
        "      i = i+1\n",
        "      if i%33 == 0:\n",
        "        generate_and_save_images(generator, seed)    \n",
        "        print(\"batch no. \"+str(i))\n",
        "      if i >= 100000:\n",
        "        break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqaqY357IFsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5eecaaf-a695-4de3-898d-2fb0ffe2b4b5"
      },
      "source": [
        "train(train_dataset,10)"

        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIkSM7qphQSc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2X2Y_oSTPcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 100 #n must be a square number\n",
        "test_input = tf.random.normal([n, 100])\n",
        "predictions = generator(test_input, training=False)   \n",
        "fig = plt.figure(figsize=(100,100))\n",
        "\n",
        "for i in range((predictions.shape[0])):\n",
        "  plt.subplot(np.sqrt(n), np.sqrt(n), i+1)\n",
        "  plt.imshow(predictions[i,:,:,0] * 127.5 * 127.5) #cmap='Blues'\n",
        "\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67BN-v4hhtar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLoss():\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(gaccuracies, label='Generative loss')\n",
        "    plt.plot(daccuracies, label='Discriminative loss')    \n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend() \n",
        "plotLoss()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
